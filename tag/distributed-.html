<!DOCTYPE html>
<head>
    <meta charset="utf-8" />
    <!-- Set the viewport width to device width for mobile -->
    <meta name="viewport" content="width=device-width" />

    <title>Edge Define: To Extend - distributed-</title>

    <link rel="stylesheet" href="../theme/css/normalize.css" />
    <link rel="stylesheet" href="../theme/css/foundation.min.css" />
    <link rel="stylesheet" href="../theme/css/style.css" />
    <link rel="stylesheet" href="../theme/css/pygments.css" />	
    <script src="../theme/js/custom.modernizr.js"></script>
</head>

<body>

<!-- Nav Bar -->
<nav>
<div class="top-bar">
<div class="row">
    <div class="large-9 large-centered columns">
	    <h1><a href="..">Edge Define: To Extend</a></h1>
    </div>
</div>
</div>

<!-- Show menu items and pages -->
<div class="row">
<div class="large-9 columns">
    <ul class="button-group navigation">

    </ul>
</div>
</div>
</nav>
<!-- End Nav -->


<!-- Main Page Content and Sidebar -->
<div class="row">

    <!-- Main Blog Content -->
    <div class="large-9 columns">
        
        

            <article>
                <a href="../distributed-tensorflow.html"><h3 class="article-title">分布式 Tensorflow</h3></a>
<h6 class="subheader" title="2018-04-28T00:00:00+08:00">六 28 四月 2018
</h6>


<h1>Tensorflow 分布式训练的各种玩儿法 - 蹭 1.8 的热度</h1>
<p>Tensorflow 1.8 发布了！ 保持着差不多一个月一个版本，够可以的！
完整 Release Note 请移步 <a href="https://github.com/tensorflow/tensorflow/releases/tag/v1.8.0">Github</a>。</p>
<p>抛开 Tensorflow Lite 不说，我特别关心的是这一条：</p>
<blockquote>
<p>Can now pass <code>tf.contrib.distribute.MirroredStrategy()</code> to <code>tf.estimator.RunConfig()</code>
to run an <code>Estimator</code> model on multiple GPUs on one machine</p>
</blockquote>
<p>解读下：TF 高层（high level） API <code>Estimator</code> 通过 <code>MirroredStrategy()</code>
支持单机多卡分布式了（In-graph replication, all-reduce synchronous）。
我们有理由相信，后面应该会有更多的分布式策略被支持，多节点，异步，模型并行等。</p>
<p>而我们的目标呢，在某些场景下基于目前的机房建设肯定是多机多卡才够劲儿。
所以，今天就简单总结下，我了解的 Tensorflow 分布式训练的各种玩儿法，以及接下来会继续跟进的几个方向。</p>
<h2>1. 经典 ps &amp; worker 模式</h2>
<p><img src="./images/dist-tf/ps_worker_arch.png" alt="ps_worker_arch" style="width: 1000px;"/></p>
<p>假定大家对 Tensorflow 的一些基本<a href="https://www.tensorflow.org/programmers_guide/low_level_intro">概念</a>及<a href="https://www.tensorflow.org/extend/architecture">架构</a>已经有所了解，在开始介绍经典模式之前，
只简单介绍下分布式涉及到的一些重点概念及策略对比：</p>
<ul>
<li>
<p>模型并行 vs 数据并行</p>
<p>模型并行：模型的各个部分并行于多个计算设备上；适应场景大模型，单个设备容不下；或者模型本身有比较好的并行性；</p>
<p>数据并行：多个模型副本分别处理不同的训练数据，以提高整体吞吐量；是常见的分布式训练策略；</p>
<p><img src="./images/dist-tf/data_paralle.jpg" alt="data_parallelism" style="width: 600px;"/></p>
</li>
<li>
<p><code>in-graph</code> replication vs <code>between-graph</code> replication</p>
<p><code>in-graph</code>: 图内复制，只构建一个 client 并把包含所有的 <code>worker</code> 设备都在定义一个图中，
如果 worker 节点及设备过多，计算图过大会导致性能下降；而且只构建一个 client，数据分发的效率以及整体容错性都不好；</p>
<p><code>between-graph</code>: 图间复制，每个 <code>worker</code> 都初始化一个计算图副本， 通过 <code>ps</code>( parameter server)
共享变量参数，只需要更新参数，免除了数据分发环节，在规模较大的情况下，相比 <code>in-graph</code> 提高了训练并行效率和容错性；</p>
</li>
<li>
<p>同步训练 vs 异步训练</p>
<p>同步训练：每一次梯度更新，需要等所有的 <code>worker</code> 处理完待训练数据，先做聚会处理后再更新参数；
优势是 loss 下降稳定；劣势是每一步的处理速度都取决于最慢的那个 <code>worker</code>；</p>
<p>异步训练：各个 <code>worker</code> 的计算及模型更新都是相互独立的，没有统一控制；
优势是速度，优化计算资源利用率；劣势是 loss 下降不稳定；</p>
<p><img src="./images/dist-tf/tf_sync_async.png" alt="sync_vs_async" style="width: 600px;"/></p>
</li>
</ul>
<p>因为<code>数据同步</code>相比较<code>模型同步</code>具有更普适的应用场景，所以针对<code>数据同步</code>的分布式训练的支持也就更适合作为 Tensorflow 通用特性来在框架级支持。</p>
<p>而源于 <code>DistBelief</code> 的基于 <code>ps</code> 和 <code>worker</code> 分布式训练架构在 Tensorflow 很早的版本中便提供了支持，这也就是这里称之为 <code>经典</code> 模式的原因。</p>
<p>在 Tensorflow 的 <code>ps</code> 和 <code>worker</code> 模式下，<code>in-graph</code> 和 <code>between-graph</code> replication 都有支持，但是基于性能和实用性考虑，可能 <code>between-graph</code> 使用的更多一些，<code>同步</code>和<code>异步</code>则更多的是根据模型的实际效果以及项目的具体需求来选择。</p>
<h3>集群描述 <code>tf.train.ClusterSpec</code></h3>
<p>参考：<a href="https://www.tensorflow.org/deploy/distributed">Distributed Tensorflow</a></p>
<div class="highlight"><pre><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">ClusterSpec</span><span class="p">({</span>
    <span class="s">&quot;worker&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="s">&quot;worker0.example.com:2222&quot;</span><span class="p">,</span>
        <span class="s">&quot;worker1.example.com:2222&quot;</span><span class="p">,</span>
        <span class="s">&quot;worker2.example.com:2222&quot;</span>
    <span class="p">],</span>
    <span class="s">&quot;ps&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="s">&quot;ps0.example.com:2222&quot;</span><span class="p">,</span>
        <span class="s">&quot;ps1.example.com:2222&quot;</span>
    <span class="p">]})</span>

<span class="c"># 其中， &quot;ps&quot; 及 &quot;worker&quot; 为 `job_name`, 还需要 `task_index` 来创建具体的 `tf.train.Server` 实例。</span>
</pre></div>


<p><strong>注：</strong> 可以通过脚本或者借助调度框架来动态构建 <code>ClusterSpec</code>。</p>
<h3>Server 通讯协议（protocol）</h3>
<ul>
<li>Default <code>grpc</code></li>
<li>Verbs based RDMA with <code>grpc+verbs</code>: https://github.com/tensorflow/tensorflow/tree/v1.8.0/tensorflow/contrib/verbs</li>
<li>MPI with <code>grpc+mpi</code>: https://github.com/tensorflow/tensorflow/tree/v1.8.0/tensorflow/contrib/mpi</li>
<li>GPU Direct RDMA with <code>grpc+gdr</code>: https://github.com/tensorflow/tensorflow/tree/v1.8.0/tensorflow/contrib/gdr</li>
</ul>
<div class="highlight"><pre><span class="n">server</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Server</span><span class="p">(</span><span class="n">cluster</span><span class="p">,</span> <span class="n">job_name</span><span class="o">=</span><span class="s">&quot;local&quot;</span><span class="p">,</span> <span class="n">task_index</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">protocol</span><span class="o">=</span><span class="s">&#39;grpc+gdr&#39;</span><span class="p">)</span> <span class="c"># default protocol is &#39;grpc&#39;</span>
</pre></div>


<h3>实践中需要留意的点</h3>
<ul>
<li>同步还是异步的选择</li>
<li><code>ps</code> 和 <code>worker</code> 的个数比率的调整</li>
<li><code>ps</code> 带宽占用过高时，<code>ps</code> 及 <code>worker</code> 的调度策略</li>
<li>分布式训练的状态机定义，包括终止态定义，以及当有 <code>worker</code> 训练失败后，是否支持重启训练等</li>
</ul>
<h2>2. 高层 API (<code>Estimator</code> 和 <code>Dataset</code>)</h2>
<p>参考： <a href="https://arxiv.org/pdf/1708.02637.pdf">Tensorflow Estimator 2017</a> 及 https://medium.com/onfido-tech/higher-level-apis-in-tensorflow-67bfb602e6c0</p>
<p><img src="./images/dist-tf/estimator_stack.png" alt="estimator_stack" style="width: 600px;"/></p>
<blockquote>
<p>注： <code>Experiment</code> 已经废弃了， 具体参考： <a href="https://github.com/tensorflow/tensorflow/tree/r1.8/tensorflow/contrib/learn#experiment-class---distributed-training-tooling">Github 文档</a>。</p>
<ul>
<li>Remove the <code>experiment_fn</code>. Instead, create the <code>Estimator</code>,
<code>train_spec</code> and <code>eval_spec</code>, then call <code>tf.estimator.train_and_evaluate</code> directly.</li>
</ul>
</blockquote>
<p><code>Estimator</code> 代表一个完整的模型。Estimator API 提供的方法包括模型的训练、评估、预测及导出。
<img src="./images/dist-tf/estimator_interface.png" alt="estimator_interface" style="width: 800px;"/></p>
<blockquote>
<p><code>Estimator</code> 具有下列优势：</p>
<ul>
<li>您可以在本地主机上或分布式多服务器环境中运行基于 <code>Estimator</code> 的模型，而无需更改模型。此外，您可以在 CPU、GPU 或 TPU 上运行基于 Estimator 的模型，而无需重新编码模型。</li>
<li><code>Estimator</code> 简化了在模型开发者之间共享实现的过程。</li>
<li>您可以使用高级直观代码开发先进的模型。简言之，采用 <code>Estimator</code> 创建模型通常比采用低阶 TensorFlow API 更简单。</li>
<li><code>Estimator</code> 本身在 <code>tf.layers</code> 之上构建而成，可以简化自定义过程。</li>
<li><code>Estimator</code> 会为您构建图。也就是说，您不必构建图。</li>
<li><code>Estimator</code> 提供安全的分布式训练循环，可以控制如何以及何时：</li>
<li>构建图</li>
<li>初始化变量</li>
<li>开始排队</li>
<li>处理异常</li>
<li>创建检查点文件并从故障中恢复</li>
<li>保存 TensorBoard 的摘要</li>
</ul>
<p>使用 <code>Estimator</code> 编写应用时，您必须将数据输入管道从模型中分离出来。这种分离简化了实验不同数据集的流程。</p>
</blockquote>
<p>更多细节： https://www.tensorflow.org/programmers_guide/estimators</p>
<p><code>Dataset</code> 是训练数据的抽象，通过 <code>tf.data</code> 下的 API 可以构建数据 pipeline，轻松处理大量数据、不同的数据格式以及复杂的转换。</p>
<ul>
<li><code>class Dataset</code>: Represents a potentially large set of elements.</li>
<li><code>class FixedLengthRecordDataset</code>: A <code>Dataset</code> of fixed-length records from one or more binary files.</li>
<li><code>class Iterator</code>: Represents the state of iterating through a <code>Dataset</code>.</li>
<li><code>class TFRecordDataset</code>: A <code>Dataset</code> comprising records from one or more TFRecord files.</li>
<li><code>class TextLineDataset</code>: A <code>Dataset</code> comprising lines from one or more text files.</li>
</ul>
<p>更多细节： https://www.tensorflow.org/programmers_guide/datasets</p>
<h3>集群描述 <code>TF_CONFIG</code> 环境变量</h3>
<p>参考：<a href="https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate"><code>tf.estimator.train_and_evaluate</code> with <code>Estimator</code></a></p>
<p>chief worker（必须有且只有一个）:</p>
<div class="highlight"><pre><span class="c"># This should be a JSON string, which is set as environment variable. Usually</span>
<span class="c"># the cluster manager handles that.</span>
<span class="n">TF_CONFIG</span><span class="o">=</span><span class="s">&#39;{</span>
    <span class="s">&quot;cluster&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s">&quot;chief&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s">&quot;host0:2222&quot;</span><span class="p">],</span>
        <span class="s">&quot;worker&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s">&quot;host1:2222&quot;</span><span class="p">,</span> <span class="s">&quot;host2:2222&quot;</span><span class="p">,</span> <span class="s">&quot;host3:2222&quot;</span><span class="p">],</span>
        <span class="s">&quot;ps&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s">&quot;host4:2222&quot;</span><span class="p">,</span> <span class="s">&quot;host5:2222&quot;</span><span class="p">]</span>
    <span class="p">},</span>
    <span class="s">&quot;task&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s">&quot;type&quot;</span><span class="p">:</span> <span class="s">&quot;chief&quot;</span><span class="p">,</span> <span class="s">&quot;index&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
<span class="p">}</span><span class="s">&#39;</span>
</pre></div>


<p>evaluator task（只有一个）:</p>
<div class="highlight"><pre><span class="c"># This should be a JSON string, which is set as environment variable. Usually</span>
<span class="c"># the cluster manager handles that.</span>
<span class="n">TF_CONFIG</span><span class="o">=</span><span class="s">&#39;{</span>
    <span class="s">&quot;cluster&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s">&quot;chief&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s">&quot;host0:2222&quot;</span><span class="p">],</span>
        <span class="s">&quot;worker&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s">&quot;host1:2222&quot;</span><span class="p">,</span> <span class="s">&quot;host2:2222&quot;</span><span class="p">,</span> <span class="s">&quot;host3:2222&quot;</span><span class="p">],</span>
        <span class="s">&quot;ps&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s">&quot;host4:2222&quot;</span><span class="p">,</span> <span class="s">&quot;host5:2222&quot;</span><span class="p">]</span>
    <span class="p">},</span>
    <span class="s">&quot;task&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s">&quot;type&quot;</span><span class="p">:</span> <span class="s">&quot;chief&quot;</span><span class="p">,</span> <span class="s">&quot;index&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
<span class="p">}</span><span class="s">&#39;</span>
</pre></div>


<p><strong>注：</strong> 可以通过脚本或者借助调度框架来设置 <code>TF_CONFIG</code> 。</p>
<h3>实践中需要留意的点</h3>
<ul>
<li><code>Dataset</code> 性能： https://www.tensorflow.org/versions/master/performance/datasets_performance</li>
<li>自定义 <code>Estimator</code>: https://www.tensorflow.org/get_started/custom_estimators</li>
</ul>
<h2>3. Ring Allreduce</h2>
<p>参考：<a href="https://zhuanlan.zhihu.com/p/34172340" title="【第一期】AI Talk：TensorFlow 分布式训练的线性加速实践">【第一期】AI Talk：TensorFlow 分布式训练的线性加速实践</a></p>
<p><img src="./images/dist-tf/ring_allreduce.png" alt="ring_allreduce" style="width: 800px;"/></p>
<p>无论是经典 ps &amp; worker 模式，还是 High Level 的 <code>Estimator</code> API，在训练集群规模和模型较大时，
集中式的参数同步都会造成网络瓶颈。</p>
<p>好在 Baidu SVAIL 已经将 HPC 领域一种比较成熟的通信算法 Ring Allreduce 引入到 Deep Learning 训练框架中。</p>
<p>引入 Ring Allreduce 之后的拓扑变化如下：</p>
<p style="width: 1000px;">
<img src="./images/dist-tf/ps_worker_top.jpg" alt="ps_worker_top" style="width: 450px;"/>
&rarr;
<img src="./images/dist-tf/ring_top.jpg" alt="ring_top" style="width: 450px;"/>
</p>

<h3>参数更新步骤</h3>
<ol>
<li>生产或者定义集群的拓扑结构</li>
</ol>
<ul>
<li>GPU 集群被组织成一个逻辑环</li>
<li>每个 GPU 有一个左邻居、一个右邻居</li>
<li>每个 GPU 只从左邻居接受数据、并发送数据给右邻居</li>
</ul>
<ol>
<li>梯度融合</li>
</ol>
<ul>
<li>
<p>Scatter Reduce：在这个 Scatter Reduce阶段，GPU 会逐步交换彼此的梯度并融合，最后每个 GPU 都会包含完整融合梯度的一部分</p>
</li>
<li>
<p>Allgather：GPU 会逐步交换彼此不完整的融合梯度，最后所有 GPU 都会得到完整的融合梯度</p>
</li>
</ul>
<h4>Scatter Reduce</h4>
<blockquote>
<p>为了方便说明，我们用梯度加和代替梯度融合。假设集群中有 N 个 GPU，那么将梯度数据等分为N 份，
接下来将在 GPUs 间进行 N-1 次 Scatter Reduce迭代，在每一次迭代中，每个 GPU 都会发送所有梯度数据的 1/N 给右邻居，
并从左邻居接收所有梯度数据的 1/N 。同一次 Scatter Reduce 迭代中，发送和接收的数据块的编号是不同的，
例如，第一轮迭代，第 n 个 GPU 会发送第 n 号数据块，并接收第 n-1 号数据块。经过 n-1 轮迭代，梯度数据会像图2 所示，
每个 GPU 都包含了部分完整梯度信息。</p>
<p><img src="./images/dist-tf/scatter_reduce_1.jpg" alt="ring_top" style="width: 600px;"/>
<img src="./images/dist-tf/scatter_reduce_2.jpg" alt="ring_top" style="width: 600px;"/></p>
</blockquote>
<h4>Allgather</h4>
<blockquote>
<p>和 Scatter Reduce 阶段类似，只不过这里只拷贝不求和，最终每个GPU 都得到了所有融合后的梯度。</p>
<p><img src="./images/dist-tf/allgather_1.jpg" alt="allgather_1" style="width: 600px;"/>
<img src="./images/dist-tf/allgather_2.jpg" alt="allgather_2" style="width: 600px;"/></p>
</blockquote>
<h4>这么做有什么好处呢？</h4>
<blockquote>
<p>下面我们来定量的分析一下，每个 GPU 在Scatter Reduce 阶段，接收 N-1 次数据，N 是 GPU 数量；
每个 GPU 在allgather  阶段，接收 N-1 次 数据；每个 GPU 每次发送 K/N  大小数据块，K 是总数据大小；
所以，Data Transferred=2(N−1)<em>K/N = (2(N−1)/N)</em>K，随着 GPU 数量 N 增加，总传输量恒定！
总传输量恒定意味着通信成本不随 GPU 数量增长而增长，也就是说我们系统拥有理论上的线性加速能力。
再回到 DS2 的例子，300million 参数也就是 1.2Gb 数据量，Ring Allreduce 方式更新一次需要传送并接收 2.4Gb 数据，
假设网络使用 GPUDirect RDMA + InfiniBand，GPUDirect RDMA 带宽约为10Gb/s；InfiniBand 带宽约为 6Gb/s，
所以通信瓶颈在 InfiniBand 。(2.4Gb)/(6.0Gb/s) ≈ 400ms，也就是每轮迭代需要 400 ms 做参数同步，
这 400ms 的数据传输时间是恒定的，不随 GPU 数量增加而增加。</p>
</blockquote>
<p>划重点： <strong>随着设备数量 N 增加，总传输量恒定</strong></p>
<h3>Tensorflow allreduce 实践</h3>
<p>Tensorflow 1.8 的 Release Note 说到了，单机多卡的 allreduce 可以通过 <code>Estimator</code> + <code>tf.contrib.distribute.MirroredStrategy()</code> 实现；</p>
<p>对于多机多卡， 来自 Uber 的开源项目 <a href="https://github.com/uber/horovod">Horovod</a> 就是一个不错的选择。</p>
<p><img src="./images/dist-tf/horovod_benchmark.png" alt="horovod_benchmark" style="width: 800px;"/></p>
<h2>总结</h2>
<p>通过从经典 <code>ps</code> <code>worker</code> 模式到 <code>Estimator</code> API 封装，再到更高效的 <code>Ring Allreduce</code> 的引入，
可以看出随着 Deep Learning 需求的激增，训练框架也被朝着更快更强和更易用的方向推进。
分布式训练在大规模机器学习中是最耗时间和计算资源的哪一环，如何能有效的提高分布式训练的效率，
一定是各框架和 AI 平台提供方大家集中发力的一个点。</p>
<p>Ring Allreduce 只是优化分布式训练中参数同步的一个方向，通过 Horovod 的推出，我们也确实看到了几乎逼近理想的线性扩展能力，
但其实深度学习是个整体工程，一定还有其他更多方面等着大家去不断的优化。</p>
<p>基于容器支撑深度学习 AI 平台，可以有效的发挥容器更轻量的优势，而且镜像封装各个框架及运行环境也可以降低上手门槛，再加上像 Kubernetes
这样的容器调度平台，在提高计算资源的整合能力的同时保证整体使用率。</p>
<p><a href="https://github.com/kubeflow/kubeflow">kubeflow</a> 就是一个整合了基于 Kubernetes 生态圈的机器学习工具箱，
虽然目前还不算成熟，但还是朝着很不错的方向在前进，包括 tf-operator 的升级，更多框架的支持，更好的存储接入，引入工作流引擎等，
都是很有吸引力的尝试。</p>
<p>后面我会继续跟进 kubeflow，希望能有更多有意思的东西总结出来。</p>
<p>限于个人能力，这里只是简单“堆叠”了我学习 Tensorflow 分布式训练过程中的找到的一些资料和说明，如有错误，还望指出。</p>
<h2>Refs:</h2>
<ul>
<li>https://zhuanlan.zhihu.com/p/35083779 "分布式 TensorFlow 入门教程"</li>
<li>https://zhuanlan.zhihu.com/p/34172340 "【第一期】AI Talk：TensorFlow 分布式训练的线性加速实践"</li>
<li>https://www.oreilly.com/ideas/distributed-tensorflow</li>
<li>http://download.tensorflow.org/paper/whitepaper2015.pdf</li>
<li>https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf</li>
<li>https://arxiv.org/pdf/1708.02637.pdf</li>
<li>https://eng.uber.com/horovod/  "Meet Horovod: Uber’s Open Source Distributed Deep Learning Framework for TensorFlow"</li>
<li>https://medium.com/onfido-tech/higher-level-apis-in-tensorflow-67bfb602e6c0 "Higher-Level APIs in TensorFlow"</li>
</ul><p class="subheader">Category: <a href="../category/ai.html">AI</a>

    Tagged: 
    <a href="../tag/ai.html">ai </a>
    <a href="../tag/tensorflow.html">tensorflow </a>
    <a href="../tag/distributed-.html">distributed- </a>
</p>



            </article>

<div class="pagination-centered">
<h6 class="subheader">Page 1 of 1</h6>

<p>

</p>
</div>



            <!-- /#posts-list -->

    </div>
    <!-- End Main Content -->

    <!-- Sidebar -->
    <aside class="large-3 columns">
        <h5 class="sidebar-title">Site</h5>
        <ul class="side-nav">
            <li><a href="../archives.html">Archives</a>
            <li><a href="../tags.html">Tags</a>
        </ul>

		
        <h5 class="sidebar-title">Categories</h5>
        <ul class="side-nav">
            <li><a href="../category/ai.html">AI</a></li>
            <li><a href="../category/bigdata.html">bigdata</a></li>
            <li><a href="../category/git.html">git</a></li>
            <li><a href="../category/others.html">others</a></li>
   
        </ul>

        <h5 class="sidebar-title">Social</h5>
        <ul class="side-nav">
            <li><a href="https://github.com/xychu">GitHub</a></li>
            <li><a href="http://weibo.com/learn2live">Weibo</a></li>
        </ul>


        <h5 class="sidebar-title">Links</h5>
        <ul class="side-nav">
            <li><a href="http://book.douban.com/people/58301079/">豆瓣读书主页</a></li>
        </ul>

    </aside> <!-- End Sidebar -->

</div> <!-- End Main Content and Sidebar -->


<!-- Footer -->
<footer class="row">
    <div class="large-12 columns">
        <hr />
        <div class="row">
            <div class="large-6 columns">
              <!--                  <p>Edge Define: To Extend by xychu</p> -->
                  <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">
                      <img alt="Creative Commons License" style="border-width:0" src="http://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" />
                  </a>
            </div>
            </div>
    </div>
</footer>
<a href="http://github.com/xychu/xychu_pelican">
<img style="position: absolute; top: 0; right: 0; border: 0;" src="http://s3.amazonaws.com/github/ribbons/forkme_right_orange_ff7600.png" alt="Fork me on GitHub" />
</a>
<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>